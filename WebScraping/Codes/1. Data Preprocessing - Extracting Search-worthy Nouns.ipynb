{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "Regexp_tokenizing=RegexpTokenizer(\"\\w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip=pd.read_csv(r'..\\Reddit_Comments\\Final_reddits_Jul13.csv',index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Author_Name', 'Content', 'Type_of_Content', 'Time',\n",
       "       'Main_Document_id', 'url', 'current_car'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ip.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip.dropna(subset=['Content'],inplace=True)\n",
    "ip.index=range(len(ip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "List_vehicle_considered=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n"
     ]
    }
   ],
   "source": [
    "for each in range(len(ip)):\n",
    "    if each%100==0: print (each)\n",
    "    if re.search(r'vehicle.*?considered.{0,2}:(.*?)(\\.|:)',ip.ix[each,'Content'],re.I):\n",
    "        text=re.search(r'vehicle.*?considered.{0,2}:(.*?)(\\.|:)',ip.ix[each,'Content'],re.I).group(1)\n",
    "        if re.search(r'(?=.*,)(?=.*\\()',text): \n",
    "            text=re.sub(re.compile(r'(\\(.*?\\))',re.I),' ',text)\n",
    "            List_vehicle_considered.extend(text.split(','))\n",
    "        else:\n",
    "            List_vehicle_considered.extend(text.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3493"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(List_vehicle_considered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'All of the Subarus',\n",
       " ' Golf TDI',\n",
       " ' Mazda CX5',\n",
       " 'Miata  ',\n",
       " ' mr2 spyder  ',\n",
       " ' Fox body mustang',\n",
       " 'Toyota Corolla IM',\n",
       " ' Honda Civic',\n",
       " ' Ford Escape',\n",
       " 'Scion xB',\n",
       " ' Scion iM',\n",
       " ' Hyundai Veloster',\n",
       " ' Hyundai Genesis',\n",
       " ' Toyotya Corolla']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "List_vehicle_considered[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "List_vehicle_considered=[each.lower().strip() for each in List_vehicle_considered if len(each)>1]\n",
    "List_vehicle_considered=list(set(List_vehicle_considered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2044"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(List_vehicle_considered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip['current_car'].fillna('EMPTY',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n"
     ]
    }
   ],
   "source": [
    "List_vehicle_owned=[]\n",
    "for each in range(len(ip)):\n",
    "    if each%100==0: print (each)\n",
    "    if ip.loc[each,'current_car']!='EMPTY':\n",
    "        temp_list=re.split(r',|\\|',ip.loc[each,'current_car'])\n",
    "        List_vehicle_owned.extend(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "List_vehicle_owned=[each.lower().strip() for each in List_vehicle_owned]\n",
    "List_vehicle_owned=list(set(List_vehicle_owned))\n",
    "List_vehicle_owned=[each for each in List_vehicle_owned if len(each)>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "802"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(List_vehicle_owned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "List_vehicles=List_vehicle_owned+List_vehicle_considered\n",
    "List_vehicles=list(set(List_vehicles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2820"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(List_vehicles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1990 eagle talon tsi awd',\n",
       " \"85' rx7 gs\",\n",
       " '2nd gen nissan pathfinder',\n",
       " 'mazda3 5-door',\n",
       " '190e']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "List_vehicles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(List_vehicles,open(\"vehicle_models.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "?re.split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 ['All', 'of', 'the', 'Subarus']\n",
      "6 ['I', 've', 'driven', 'one', 'and', 'while', 'they', 're', 'not', 'supposed', 'to', 'be', 'drag', 'monsters']\n",
      "6 ['I', 'kinda', 'want', 'something', 'a', 'little', 'faster']\n",
      "6 ['mr2', 'spyder', 'owned', 'one', 'but', 'engine', 'blew']\n",
      "6 ['kind', 'of', 'scared', 'me', 'off', 'although', 'I', 'love', 'it']\n"
     ]
    }
   ],
   "source": [
    "for each in range(len(ip[0:100])):\n",
    "    if re.search(r'vehicle.*?considered.{0,2}:(.*?)\\.',ip.ix[each,'Content'],re.I):\n",
    "        text=re.search(r'vehicle.*?considered.{0,2}:(.*?)\\.',ip.ix[each,'Content'],re.I).group(1)\n",
    "        if re.search(r',',text): \n",
    "            text2=text.split(',')\n",
    "            text3=[Regexp_tokenizing.tokenize(element) for element in text2]\n",
    "            for element in text3:\n",
    "                if len(element)>3:\n",
    "                    print (each, element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Location:California. Price Range:18k-25k. Buy. New. Sedan or crossover. Must Hve:Fuel efficient, AWD, Bluetooth, navigation. Automatic. Daily driver. Vehicle considered:Toyota Corolla IM, Honda Civic, Ford Escape. Minor Work:Change oil, battery No major work. Pretty much novice when it comes to vehicles, appreciate any advice. '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ip.loc[15,'Content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pprint\n",
    "from nltk import Tree\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern=r\"\"\"\n",
    "NP:\n",
    "    {<NN.*>+<CD>*}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_NPChunker = nltk.RegexpParser(pattern)\n",
    "Regexp_tokenizing=RegexpTokenizer(\"\\w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('all.a.01'), Synset('all.s.02'), Synset('wholly.r.01')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "text=\"Lexus Ct200h, Mazda CX-3, Acura ILX/TLX\"\n",
    "word_list=text.split()\n",
    "for each in word_list:\n",
    "    print (wordnet.synsets(each))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I've\", 'had', 'a', 'Honda', 'Accord,', 'Nissan', 'Maxima,', 'Nissan', 'Rogue,', 'Honda', 'Odyssey', '-', 'all', '3', 'years', 'each']\n",
      "[(\"I've\", 'NNP'), ('had', 'VBD'), ('a', 'DT'), ('Honda', 'NNP'), ('Accord,', 'NNP'), ('Nissan', 'NNP'), ('Maxima,', 'NNP'), ('Nissan', 'NNP'), ('Rogue,', 'NNP'), ('Honda', 'NNP'), ('Odyssey', 'NNP'), ('-', ':'), ('all', 'DT'), ('3', 'CD'), ('years', 'NNS'), ('each', 'DT')]\n"
     ]
    }
   ],
   "source": [
    "text=\"I\\'ve had a Honda Accord, Nissan Maxima, Nissan Rogue, Honda Odyssey - all 3 years each\"\n",
    "word_list=text.split()\n",
    "print (word_list)\n",
    "print (nltk.pos_tag(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
